{"classifiers": ["Intended Audience :: Developers", "Intended Audience :: Science/Research", "Intended Audience :: Information Technology", "Topic :: Scientific/Engineering", "Topic :: Scientific/Engineering :: Image Recognition", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Software Development :: Libraries", "Topic :: Utilities", "License :: OSI Approved :: BSD License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Environment :: Console", "Natural Language :: English", "Operating System :: Linux"], "extensions": {"python.commands": {"wrap_console": {"model-analyzer": "model_analyzer.entrypoint:main"}}, "python.details": {"contacts": [{"email": "sw-dl-triton@nvidia.com", "name": "NVIDIA Inc.", "role": "author"}], "document_names": {"description": "DESCRIPTION.rst"}, "project_urls": {"Home": "https://developer.nvidia.com/nvidia-triton-inference-server"}}, "python.exports": {"console_scripts": {"model-analyzer": "model_analyzer.entrypoint:main"}}}, "extras": [], "generator": "bdist_wheel (0.30.0)", "keywords": ["triton", "tensorrt", "inference", "server", "service", "analyzer", "nvidia"], "license": "BSD", "metadata_version": "2.0", "name": "triton-model-analyzer", "run_requires": [{"requires": ["cryptography (>=3.3.2)", "distro (>=1.5.0)", "docker (>=4.3.1)", "httplib2 (>=0.19.0)", "matplotlib (>=3.3.4)", "numba (>=0.51.2)", "pdfkit (>=0.6.1)", "prometheus-client (>=0.9.0)", "psutil (>=5.8.0)", "pyyaml (>=5.3.1)", "requests (>=2.24.0)", "tritonclient[all] (>=2.4.0)"]}], "summary": "The Model Analyzer is a tool to analyze the runtime performance of one or more models on the Triton Inference Server", "version": "1.7.0"}