# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['dagger',
 'dagger.dag',
 'dagger.data_structures',
 'dagger.dsl',
 'dagger.input',
 'dagger.output',
 'dagger.runtime.argo',
 'dagger.runtime.cli',
 'dagger.runtime.local',
 'dagger.serializer',
 'dagger.task']

package_data = \
{'': ['*']}

entry_points = \
{'console_scripts': ['argo-specific-extras = '
                     'examples.argo_specific_extras:run_from_cli',
                     'composite-map-reduce = '
                     'examples.composite_map_reduce:run_from_cli',
                     'hello-world = examples.hello_world:run_from_cli',
                     'map-reduce = examples.map_reduce:run_from_cli',
                     'nested-dags = examples.nested_dags:run_from_cli',
                     'nested-map-reduce = '
                     'examples.nested_map_reduce:run_from_cli',
                     'passing-parameters = '
                     'examples.passing_parameters:run_from_cli']}

setup_kwargs = {
    'name': 'py-dagger',
    'version': '0.1.0',
    'description': 'Define sophisticated data pipelines with Python and run them on different distributed systems (such as Argo Workflows).',
    'long_description': '# Dagger\n\nDefine sophisticated data pipelines and run them on different distributed systems (such as Argo Workflows).\n\n![Python Versions Supported](https://img.shields.io/badge/python-3.8+-blue.svg)\n[![Latest PyPI version](https://badge.fury.io/py/py-dagger.svg)](https://badge.fury.io/py/py-dagger)\n[![Test Coverage (Codecov)](https://codecov.io/gh/larribas/dagger/branch/main/graph/badge.svg?token=fKU68xYUm8)](https://codecov.io/gh/larribas/dagger)\n![QA: Tests](https://github.com/larribas/dagger/actions/workflows/tests.yaml/badge.svg)\n![QA: Documentation](https://github.com/larribas/dagger/actions/workflows/documentation.yaml/badge.svg)\n![QA: Type System](https://github.com/larribas/dagger/actions/workflows/linting.yaml/badge.svg)\n![QA: Formatting](https://github.com/larribas/dagger/actions/workflows/formatting.yaml/badge.svg)\n\n---\n\n_Dagger_ is a Python library that allows you to:\n\n* Define sophisticated DAGs (direct acyclic graphs) using very straightforward Pythonic code.\n* Run those DAGs seamlessly in different runtimes or workflow orchestrators (such as Argo Workflows, Kubeflow Pipelines, and more).\n\n\n## _Dagger_ in Action\n\nThis section shows a couple of examples of what _Dagger_ is capable of. Our [official documentation](https://dagger.readthedocs.io) contains a breadth of tutorials, examples, recommendations and API references. Make sure to check it out!\n\n\n### Installing the library\n\n_Dagger_ is published to the Python Package Index (PyPI). To install it, you can simply run:\n\n```\npip install py-dagger\n```\n\n\n### Hello World - Tasks and DAGs\n\nThe following example shows how to run a simple hello world using the local runtime:\n\n\n```python\nfrom dagger.dsl import task, DAG, build\nfrom dagger.runtime.local import invoke\n\n@task\ndef say_hello_world():\n    print("hello world!")\n\n@DAG\ndef hello_world_pipeline():\n    say_hello_world()\n\ndag = build(hello_world_pipeline)\ninvoke(dag)\n```\n\nRunning this will print `"hello world!"`.\n\nWhile not particularly interesting, this example shows the basic building blocks of _DAGger_: __Tasks__ and __DAGs (directed acyclic graphs)__. DAGs contain a series of nodes connected together via their inputs/outputs. Nodes may be Tasks (a Python function wrapped with some extra metadata) or other DAGs.\n\nIt also shows how we can define DAGs in an imperative, Pythonic style, build them (i.e. turning them into a data structure representing the DAG) and run them using one of our runtimes (in this case, the local runtime, which will just run it in memory).\n\nHungry for more? Let\'s take a look at a more complex example.\n\n\n### Map-Reduce Operations - Parameters and parallelization\n\nThe following example generates a list of numbers. The length of the list varies randomly. Then, in parallel, we transform/map each of these numbers raising them to a power we receive as a parameter. Finally, we sum all the results and produce a single output.\n\n\n```python\nimport random\n\n@task\ndef generate_numbers():\n    length = random.randint(3, 20)\n    numbers = list(range(length))\n    print(f"Generating the following list of numbers: {numbers}")\n    return numbers\n\n@task\ndef raise_number(n, exponent):\n    print(f"Raising {n} to a power of {exponent}")\n    return n ** exponent\n\n@task\ndef sum_numbers(numbers):\n    print(f"Calculating the sum of {numbers}")\n    return sum(numbers)\n\n@DAG\ndef map_reduce_pipeline(exponent):\n    return sum_numbers(\n        [\n            raise_number(n=partition, exponent=exponent)\n            for partition in generate_numbers()\n        ]\n    )\n\ndag = build(map_reduce_pipeline)\nresult = invoke(dag, params={"exponent": 2})\nprint(f"The final result was {result}")\n```\n\n\nThis type of parallel fan-out and fan-in operations are very common when modelling data pipelines. _Dagger_ allows you to write them as you would in plain Python and run them on a number of distributed systems.\n\n\n### Built-in and Custom Serializers\n\nOne of the things you may notice is that the result of running the DAG is not of type `int`, but of type `bytes`. This is because a node produces results in their __serialized format__. This may look like an odd choice when running things locally, but keep in mind that the final goal of the library is to be able to run each step of your DAG in different machines over the network.\n\n_Dagger_ helps you connect the outputs of some nodes to the inputs of other nodes, but to do so, these pieces of information have to travel as bytes through the network, and be serialized/deserialized when leaving/entering the Python code.\n\n__The default serialization format is JSON__, but in practice, you will find yourself using other kinds of serialization formats that fit your use case better.\n\nFor instance, when dealing with Pandas `DataFrame`s, you may want to serialize those data frames as CSV or Parquet files. You can do that easily via type annotations:\n\n```python\n# ...\nimport pandas as pd\nfrom typing import Dict\nfrom dagger.dsl import task, Serialize\nfrom my_custom_serializers import AsParquet\n\n@task(serializer=Serialize(training=AsParquet(), testing=AsParquet()))\ndef split_training_test_datasets(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n    # ...\n    return {\n      "training": training_dataset,\n      "testing": testing_dataset,\n    }\n\n# ...\n```\n\nYour function code just works with Python data types, but under the hood, _Dagger_ is using the serializers you provide to pass information from one node to the other.\n\nYou can check the serializers we provide out of the box in our documentation, under [Serializers](TODO). And you can bring your own serializers very easily.\n\n\n\n### Running your DAG on a distributed runtime\n\nSo far, we have run our DAGs using the `runtime.local` package. Next, you can try exporting your DAG for execution in any of the following runtimes:\n\n* [Argo Workflows](TODO)\n\n\n\n## Design Principles and Features\n\nThe main goal of _Dagger_ is to provide a __simple yet powerful framework__ to define data/ML pipelines with __minimal friction or boilerplate__ and __prepare them to run on multiple distributed runtimes__.\n\nThe features the library provides are based on these 3 guiding principles:\n\n\n### 1. High-level abstraction for the most common use cases; Extensibility for more specific use cases\n\n_Dagger_ provides out-of-the-box support for common patterns such as:\n\n- Passing arguments from one node to another.\n- Parameterizing a DAG so its behavior can change dynamically when it\'s executed.\n- Running multiple nodes in parallel.\n- Encapsulating common behavior and composing several DAGs together.\n- Creating map-reduce operations (also known as "scatter and gather" or "dynamic fan-out and fan-in" operations).\n\n\n_Dagger_ also removes the need to explicitly serialize/deserialize data, or interface with local or remote filesystems. Users write Python code and deal with Python-native data types. That\'s it.\n\nWe believe the majority of the use cases can be built using the existing feature set. However, we also believe users should still be able to __leverage the features that make each runtime unique__. For instance:\n\n* When running on Kubernetes, users may want to fine-tune some scheduling directives.\n* When running on Argo Workflows, users may want to set a retry policy, or use memoization on their steps.\n* When developing data pipelines, users may want to use Parquet as a serialization format.\n* Users may want to create a new runtime that is not yet supported by the library.\n\n_Dagger_ is designed for extensibility. When you need to take your DAGs to the next step, you will not find yourself fighting against the framework.\n\n\n### 2. Soft learning curve\n\nWe believe if you are already fluent with Python, you should be able to pick up _Dagger_ in a couple of hours.\n\nTo soften the learning curve, we\'ve worked hard on:\n\n* A [documentation portal](https://dagger.readthedocs.io) with tutorials, recommendations and API references.\n* A comprehensive [set of examples](https://github.com/larribas/dagger/tree/main/examples), from beginner to advanced use cases.\n* Thorough error handling to catch any potential issue as early as possible. Error messages are descriptive, point you to the specific component that is causing a problem, explain the reason why it\'s failing and suggest alternatives.\n\n\n\n### 3. Performant and reliable\n\n_Dagger_ enables you to create and iterate on complex workflows. During this effort, the library should never be a limiting factor in terms of performance or reliability. That is, we want to make sure you don\'t experience any bugs, memory leaks or conflicts that impair your productivity. Hence, we have put a lot of focus on:\n\n- __Test coverage__ for internal components. _Dagger_ will always have >95% test coverage for all success and error scenarios.\n- __Zero dependencies__. When you install _Dagger_, it doesn\'t bring any other dependency with it. Your requirements file will be clean and conflict-free with other versions of other libraries.\n- __Lazy loading of input files__. Where possible, _Dagger_ will minimize the memory footprint by using lazy loading of files from local or remote filesystems into memory. This is especially useful when dealing with partitioned outputs and reduce operations.\n- __Local verification__ of your DAGs. When you build a DAG, we enforce a series of rules that make your pipelines clear and predictable. You can also execute any of your DAGs locally with the local runtime.\n\n\n## How to contribute\n\nDo you have some feedback about the library? Have you implemented a Serializer or a Runtime that may be useful for the community? Do you think a tutorial or example could be improved?\n\nEvery contribution to _Dagger_ is greatly appreciated.\n\nPlease read our [Contribution Guidelines](CONTRIBUTING.md) for more details.\n\n\n\n### Local development\n\nWe use Poetry to manage the dependencies of this library. In the codebase, you will find a `Makefile` with some useful commands to run and test your contributions. Namely:\n\n- `make install` - Install the project\'s dependencies\n- `make test` - Run tests and report test coverage. It will fail if coverage is too low.\n- `make ci` - Run all the quality checks we run for each commit/PR. This includes type hint checking, linting, formatting and documentation.\n- `make build` - Build the project\'s WHEEL\n- `make docker-build` - Package the project in a Docker image\n- `make docker-run-example-name` - Run any example DAG of those defined in "dagger/examples" (for instance, `make run-hello-world`)\n- `make set-up-argo` - Create a k3d cluster for the project and installs Argo 3.0 in it. It also sets up a k3d image registry so that you can run the examples remotely.\n- `make docker-push-local` - Build and push the project\'s Docker image to the local k3d registry.\n- `make tear-down-argo` - Destroy the k3d cluster and registry where we deployed Argo.\n',
    'author': 'larribas',
    'author_email': 'lorenzo.s.arribas@gmail.com',
    'maintainer': None,
    'maintainer_email': None,
    'url': 'https://github.com/larribas/dagger',
    'packages': packages,
    'package_data': package_data,
    'entry_points': entry_points,
    'python_requires': '>=3.8,<4.0',
}


setup(**setup_kwargs)
