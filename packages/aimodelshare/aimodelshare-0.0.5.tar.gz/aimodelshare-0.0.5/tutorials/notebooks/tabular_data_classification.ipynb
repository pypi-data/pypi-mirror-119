{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tabular_data_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BviI4QIdWiq",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img width=\"50%\" src=\"https://aimodelsharecontent.s3.amazonaws.com/aimodshare_banner.jpg\" /></p>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYNp684Tk3tb",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><h1 align=\"center\">Titanic Dataset Classification Tutorial</h1> <h3 align=\"center\">(Prepare to deploy model and preprocessor to REST API/Web Dashboard in four easy steps...)</h3></p>\n",
        "<p align=\"center\"><img width=\"80%\" src=\"https://aimodelsharecontent.s3.amazonaws.com/ModelandPreprocessorObjectPreparation.jpeg\" /></p>\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyskXyNqmBgy",
        "colab_type": "text"
      },
      "source": [
        "## **(1) Preprocessor Function & Setup**\n",
        "\n",
        "> ### A more advanced example demonstrating the flexibility of a new *Column Transformer* approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oemrLaHgs2OL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# Read data from Titanic dataset.\n",
        "titanic_url = ('https://raw.githubusercontent.com/amueller/'\n",
        "               'scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv')\n",
        "data = pd.read_csv(titanic_url)\n",
        "\n",
        "# We will train our classifier with the following features:\n",
        "# Numeric Features:\n",
        "# - age: float.\n",
        "# - fare: float.\n",
        "# Categorical Features:\n",
        "# - embarked: categories encoded as strings {'C', 'S', 'Q'}.\n",
        "# - sex: categories encoded as strings {'female', 'male'}.\n",
        "# - pclass: ordinal integers {1, 2, 3}.\n",
        "\n",
        "# We create the preprocessing pipelines for both numeric and categorical data.\n",
        "numeric_features = ['age', 'fare']\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "categorical_features = ['embarked', 'sex', 'pclass']\n",
        "\n",
        "# Replacing missing values with Modal value and then one-hot encoding.\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Final preprocessor object set up with ColumnTransformer...\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "\n",
        "X = data.drop('survived', axis=1)\n",
        "X = data.drop('name', axis=1)\n",
        "y = data['survived']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "preprocess = preprocess.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML3qhrFI9fRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessor(data):\n",
        "    preprocessed_data=preprocess.transform(data)\n",
        "    return preprocessed_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfQPzGdG92bt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7ca5c73-8dbb-4d8c-b5e7-3a56aa4c197d"
      },
      "source": [
        "preprocessor(X_train).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1047, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr19F1vtM8wy",
        "colab_type": "text"
      },
      "source": [
        "## **(2) Build Your Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b6cKXx2-SNy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "b022367d-ef1c-4b96-fff7-82685ce0807d"
      },
      "source": [
        "# Predicting whether a person survived...\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=10, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compile model...\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fitting the ANN to the Training set...\n",
        "model.fit(preprocessor(X_train), y_train, batch_size=60, epochs=7, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "18/18 - 0s - loss: 0.6346 - accuracy: 0.7011\n",
            "Epoch 2/7\n",
            "18/18 - 0s - loss: 0.5458 - accuracy: 0.7784\n",
            "Epoch 3/7\n",
            "18/18 - 0s - loss: 0.4935 - accuracy: 0.7727\n",
            "Epoch 4/7\n",
            "18/18 - 0s - loss: 0.4673 - accuracy: 0.7841\n",
            "Epoch 5/7\n",
            "18/18 - 0s - loss: 0.4562 - accuracy: 0.7966\n",
            "Epoch 6/7\n",
            "18/18 - 0s - loss: 0.4497 - accuracy: 0.7985\n",
            "Epoch 7/7\n",
            "18/18 - 0s - loss: 0.4448 - accuracy: 0.8013\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f036d6c4cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsTPVYEzNF7Z",
        "colab_type": "text"
      },
      "source": [
        "## **(3) Save Preprocessor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWlJI6_ZNHov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! pip3 install aimodelshare"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoctDjXtNHx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def export_preprocessor(preprocessor_function, filepath):\n",
        "    import dill\n",
        "    with open(filepath, \"wb\") as f:\n",
        "        dill.dump(preprocessor_function, f)\n",
        "\n",
        "# import aimodelshare as ai # Once we can deploy this, we use it in lieu of the below.\n",
        "# ai.export_preprocessor(preprocessor, \"preprocessor.pkl\")\n",
        "\n",
        "export_preprocessor(preprocessor, \"preprocessor.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9XaPH3SNIio",
        "colab_type": "text"
      },
      "source": [
        "## **(4) Save Keras Model to Onnx File Format**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTa6dITsNM-1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "e1d7225d-93fe-4c7d-9490-9cd0e4101700"
      },
      "source": [
        "! pip3 install keras2onnx\n",
        "! pip3 install onnxruntime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras2onnx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/2f/c7aef8f8215c62d55ea05f5b36737c1726e4fea6c73970909523ae497fd9/keras2onnx-1.7.0-py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (1.18.5)\n",
            "Collecting fire\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (3.12.4)\n",
            "Collecting onnxconverter-common>=1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/7a/7e30c643cd7d2ad87689188ef34ce93e657bd14da3605f87bcdbc19cd5b1/onnxconverter_common-1.7.0-py2.py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.6MB/s \n",
            "\u001b[?25hCollecting onnx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/ee/bc7bc88fc8449266add978627e90c363069211584b937fd867b0ccc59f09/onnx-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (2020.6.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire->keras2onnx) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire->keras2onnx) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->keras2onnx) (49.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx->keras2onnx) (3.7.4.3)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=d99ffeeb52ab0317dd8306a44c8d26a67c37f41e618148b15eb1d3107973cbff\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, onnx, onnxconverter-common, keras2onnx\n",
            "Successfully installed fire-0.3.1 keras2onnx-1.7.0 onnx-1.7.0 onnxconverter-common-1.7.0\n",
            "Collecting onnxruntime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/a6/30c6c17524d6930df677412eda0b3c2acd9062697fc0e86842f2dd058835/onnxruntime-1.4.0-cp36-cp36m-manylinux2010_x86_64.whl (4.4MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from onnxruntime) (1.18.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnxruntime) (3.12.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime) (49.6.0)\n",
            "Installing collected packages: onnxruntime\n",
            "Successfully installed onnxruntime-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8s1QKtoNNTz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "004f1641-53bd-420a-c54f-f3ca9c91289b"
      },
      "source": [
        "# Save model to onnx file...\n",
        "\n",
        "import os\n",
        "os.environ['TF_KERAS'] = '1' # Add this environmental variable whenever you use tensorflow's tf.keras to build your keras model.\n",
        "\n",
        "import onnx\n",
        "import keras2onnx\n",
        "\n",
        "# Convert model to onnx object\n",
        "import onnx\n",
        "from keras2onnx import convert_keras\n",
        "onnx_model = convert_keras(model, 'my_model.onnx')\n",
        "\n",
        "# Save model to local .onnx file\n",
        "with open(\"my_model.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf executing eager_mode: True\n",
            "tf.keras model eager_mode: False\n",
            "The ONNX operator number change on the optimization: 13 -> 9\n",
            "The maximum opset needed by this model is only 9.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
