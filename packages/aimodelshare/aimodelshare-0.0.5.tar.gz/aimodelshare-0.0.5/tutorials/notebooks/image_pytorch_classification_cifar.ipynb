{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_pytorch_classification_cifar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6a60c57e5cbd4bb086d03c07654bd2f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e141761a3ab44598b222ec6be17589bc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c0bbc271e6cb4370b0b5a739e3ecafdc",
              "IPY_MODEL_ef267084307c47bdb55aa12ccac3e841"
            ]
          }
        },
        "e141761a3ab44598b222ec6be17589bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0bbc271e6cb4370b0b5a739e3ecafdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_82a304d68d5647fca19004a816ef89b5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec3cfc0d820346a88f817c83fa17dcca"
          }
        },
        "ef267084307c47bdb55aa12ccac3e841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0e4a05f6f2104036b87bd6c927da89d7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:05&lt;00:00, 31042538.90it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6490fd3c7d2443d19fad9269362aca87"
          }
        },
        "82a304d68d5647fca19004a816ef89b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec3cfc0d820346a88f817c83fa17dcca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0e4a05f6f2104036b87bd6c927da89d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6490fd3c7d2443d19fad9269362aca87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BviI4QIdWiq"
      },
      "source": [
        "<p align=\"center\"><img width=\"50%\" src=\"https://aimodelsharecontent.s3.amazonaws.com/aimodshare_banner.jpg\" /></p>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuAkm2rldeUv"
      },
      "source": [
        "<p align=\"center\"><h1 align=\"center\">Image Classification with PyTorch</h1> <h3 align=\"center\">(Prepare to deploy model and preprocessor to REST API/Web Dashboard in four easy steps...)</h3></p>\n",
        "<p align=\"center\"><img width=\"80%\" src='https://drive.google.com/thumbnail?id=1ea5R66cqAXs3g4oIeEiUUMhictrL2IBg&sz=w1200-h1200' /></p>\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBoJu8oUk8UU"
      },
      "source": [
        "## **(1) Train Model Using `torch`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWt5CNFznF64"
      },
      "source": [
        "We use the standard CIFAR-10 dataset, and train it just the way it is done in the eponymous CIFAR-10 `torch` tutorial. The caveat is that we will be loading images in `batch_size=1` inside the `DataLoader()`. NB CIFAR-10 has images of size 3x32x32 (3-channel/color images of 32x32 pixels in size)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsl62dqMC92n"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqQqCYu9p2yS"
      },
      "source": [
        "`torch` datasets are PILImage images of range [0, 1]. `ToTensor()` transforms to tensors of normalized range [-1, 1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg7Rcs0rTfmi",
        "outputId": "70df2ed0-e8c7-4056-b02c-2c94902dfbd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "6a60c57e5cbd4bb086d03c07654bd2f6",
            "e141761a3ab44598b222ec6be17589bc",
            "c0bbc271e6cb4370b0b5a739e3ecafdc",
            "ef267084307c47bdb55aa12ccac3e841",
            "82a304d68d5647fca19004a816ef89b5",
            "ec3cfc0d820346a88f817c83fa17dcca",
            "0e4a05f6f2104036b87bd6c927da89d7",
            "6490fd3c7d2443d19fad9269362aca87"
          ]
        }
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, # 50000 items.\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1, # 10000 items.\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a60c57e5cbd4bb086d03c07654bd2f6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75rEs_sgTrx6",
        "outputId": "e0690f9d-237e-445e-de72-ba79351c1b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Functions to show an image.\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5 # Unnormalize.\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Get some random training images.\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# Show images.\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# Print labels.\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(1)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXRkKwgwqQtC"
      },
      "source": [
        "Let's do a plain vanilla neural net that takes our color (3-channel) images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2g3ycuQT0BT"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS9yFzsRT3s-"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # Define a Loss function.\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # Define an optimizer."
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viVOPzl_tE2I"
      },
      "source": [
        "Loop over the data iterator, feed the inputs to the network and optimize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfcf4TciTfpN",
        "outputId": "54e509ba-d261-4f8e-81bb-6e8912377118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "for epoch in range(2):  # Loop over the dataset multiple times.\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # Get the inputs; data is a list of [inputs, labels].\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero the parameter gradients.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize.\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999: # Print every 2000 mini-batches.\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 2.235\n",
            "[1,  4000] loss: 2.040\n",
            "[1,  6000] loss: 1.917\n",
            "[1,  8000] loss: 1.822\n",
            "[1, 10000] loss: 1.776\n",
            "[1, 12000] loss: 1.759\n",
            "[1, 14000] loss: 1.738\n",
            "[1, 16000] loss: 1.711\n",
            "[1, 18000] loss: 1.677\n",
            "[1, 20000] loss: 1.645\n",
            "[1, 22000] loss: 1.649\n",
            "[1, 24000] loss: 1.640\n",
            "[1, 26000] loss: 1.655\n",
            "[1, 28000] loss: 1.562\n",
            "[1, 30000] loss: 1.593\n",
            "[1, 32000] loss: 1.571\n",
            "[1, 34000] loss: 1.566\n",
            "[1, 36000] loss: 1.555\n",
            "[1, 38000] loss: 1.595\n",
            "[1, 40000] loss: 1.586\n",
            "[1, 42000] loss: 1.578\n",
            "[1, 44000] loss: 1.527\n",
            "[1, 46000] loss: 1.560\n",
            "[1, 48000] loss: 1.506\n",
            "[1, 50000] loss: 1.573\n",
            "[2,  2000] loss: 1.499\n",
            "[2,  4000] loss: 1.447\n",
            "[2,  6000] loss: 1.499\n",
            "[2,  8000] loss: 1.478\n",
            "[2, 10000] loss: 1.493\n",
            "[2, 12000] loss: 1.497\n",
            "[2, 14000] loss: 1.478\n",
            "[2, 16000] loss: 1.501\n",
            "[2, 18000] loss: 1.492\n",
            "[2, 20000] loss: 1.480\n",
            "[2, 22000] loss: 1.474\n",
            "[2, 24000] loss: 1.496\n",
            "[2, 26000] loss: 1.503\n",
            "[2, 28000] loss: 1.445\n",
            "[2, 30000] loss: 1.508\n",
            "[2, 32000] loss: 1.508\n",
            "[2, 34000] loss: 1.496\n",
            "[2, 36000] loss: 1.436\n",
            "[2, 38000] loss: 1.516\n",
            "[2, 40000] loss: 1.431\n",
            "[2, 42000] loss: 1.489\n",
            "[2, 44000] loss: 1.468\n",
            "[2, 46000] loss: 1.487\n",
            "[2, 48000] loss: 1.526\n",
            "[2, 50000] loss: 1.426\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q06ktW4HlRCu"
      },
      "source": [
        "## **(2) Save Model to ONNX**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSuCVjabPqt_"
      },
      "source": [
        "model = Net()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu2XmrCGR6cV",
        "outputId": "6ae65427-efa2-48bb-f9fd-e9a9eb00fd7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb88jVmUO1UH"
      },
      "source": [
        "dummy_input = next(iter(testloader))[0] # First observation from the batch. Tensor with shape torch.Size([1, 3, 32, 32])."
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78azKt1_OgPR"
      },
      "source": [
        "torch.onnx.export(\n",
        "    model.cpu(),\n",
        "    dummy_input,\n",
        "    'my_model.onnx',\n",
        "    export_params=True,\n",
        "    do_constant_folding=True,\n",
        ")"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBOt1rFZllpf"
      },
      "source": [
        "## **(3) Write a Preprocessor Function**\n",
        "\n",
        "> ### Preprocessor functions for image prediction models can use ***`cv2`*** and ***`numpy`*** to read in and preprocess images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj52M0FXwApr"
      },
      "source": [
        "Since we pre-process the images using simple packages, we need to read in the CIFAR-10 dataset from the previously downloaded `cifar-10-batches-py`. Since we rely on `os` and `pickle`, we do this ouside the pre-processor function. The output is an `ndarray` for the target (images) and a list for the labels. We only load in the test-set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKkWetDemtF6"
      },
      "source": [
        "img_rows, img_cols = 32, 32\n",
        "input_shape = (img_rows, img_cols, 3)\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "    \"\"\"\n",
        "    Load a single batch of CIFAR-10.\n",
        "    \"\"\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        datadict = pickle.load(f, encoding='latin1') # Python 3 and newer.\n",
        "        X = datadict['data']\n",
        "        y = datadict['labels']\n",
        "        return X, y\n",
        "\n",
        "def load_CIFAR10(ROOT):\n",
        "    \"\"\"\n",
        "    Load all, in this case, the test batch of CIFAR-10.\n",
        "    \"\"\"\n",
        "    f = os.path.join(ROOT, 'test_batch') # Load the test_batch.\n",
        "    X, y = load_CIFAR_batch(f)\n",
        "    return X, y"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m48N-l8XmtD2"
      },
      "source": [
        "cifar10_dir = '/content/data/cifar-10-batches-py' # Replace with your path.\n",
        "\n",
        "import os\n",
        "from six.moves import cPickle as pickle\n",
        "\n",
        "X, y = load_CIFAR10(cifar10_dir) # X images (targets), y labels."
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2o7NddAtWI0"
      },
      "source": [
        "`torch` tensors assume color channel is the **first** dimension, whereas `matplotlib` assumes it's the **third** dimension. Therefore always convert images (H x W x **C**) to a `torch.Tensor` of shape (**C** x H x W), and vice-versa for plotting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn4kSvNF252r"
      },
      "source": [
        "def preprocessor(target):\n",
        "        \"\"\"\n",
        "        This function preprocesses images in the form of ndarray, reshapes, normalizes and\n",
        "        converts them to float32 for ONNX before unstacking to get the requisite format for runtime.\n",
        "        \n",
        "        params:\n",
        "            target\n",
        "                loaded images as ndarray\n",
        "                      \n",
        "        returns:\n",
        "            preprocessed_X\n",
        "                preprocessed image data\n",
        "                  \n",
        "        \"\"\"\n",
        "           \n",
        "        import numpy as np\n",
        "\n",
        "        X = target # Read in the targets.\n",
        "        X = X.reshape(10000, 3, 32, 32) # Reshape to move channels last-to-first.\n",
        "        X = X.astype('float32') # float32 for ONNX.\n",
        "        X /= 255 # Normalization.\n",
        "        *X, = X # Use the star operator to un-stack a numpy array.\n",
        "        preprocessed_X = [np.expand_dims(i, axis=0) for i in X] # Adding 1 to object shape to finalize expected run-time prediction shape.                                                                            \n",
        "\n",
        "        return preprocessed_X"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJQTH9UvmtBj"
      },
      "source": [
        "testdata = preprocessor(target=X)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAwbLXqDtwD3"
      },
      "source": [
        "### **Test Model Output Using `onnxruntime`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9ThQhDDRGuL"
      },
      "source": [
        "!pip install onnxruntime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8EWIfvURPfS",
        "outputId": "bb984d4f-3c2d-45cf-c02a-d61c25ca76bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import onnxruntime as rt\n",
        "modeltest = rt.InferenceSession('my_model.onnx')\n",
        "\n",
        "input_name = modeltest.get_inputs()[0].name\n",
        "input_data=testdata[3] # Needs to be float32. Try e.g. the fourth image from testdata.\n",
        "\n",
        "res=modeltest.run(None, {input_name: input_data})\n",
        "res[0]\n",
        "prob = res[0]\n",
        "\n",
        "def predict_classes(x): # Adjusted from Keras GitHub code.\n",
        "        proba=x\n",
        "        if proba.shape[-1] > 1:\n",
        "          return proba.argmax(axis=-1)\n",
        "        else:\n",
        "          return (proba > 0.5).astype('int32')\n",
        "\n",
        "prediction_index=predict_classes(prob)\n",
        "def index_to_label(labels,index_n):\n",
        "    return labels[index_n]\n",
        "labels=['plane', 'car', 'bird', 'cat',\n",
        "        'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "result=list(map(lambda x: labels[x], prediction_index))\n",
        "\n",
        "result"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['plane']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1ZgQTsrRbcx"
      },
      "source": [
        "### **Display a Test Model Output**\n",
        "\n",
        "> #### Optional. NB The image may or may not match depending on how well the model learned on the training set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjkDt0sVXTVN"
      },
      "source": [
        "testpic = testdata[3] # Try e.g. the fourth image from testdata.\n",
        "testpic = torch.from_numpy(testpic)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwTQ8XiTvlpx"
      },
      "source": [
        "Remember, shape (H x W x **C**) for plotting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZChhhJxems3j",
        "outputId": "e8fa86c7-b54c-4890-e149-317d6fb6676c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5 # Unnormalize.\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Show images.\n",
        "imshow(torchvision.utils.make_grid(testpic))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhvNRTx4lxbc"
      },
      "source": [
        "## **(4) Save Preprocessor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xsfDZajhgbm"
      },
      "source": [
        "# ! pip3 install aimodelshare"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij0Dh8afhgYZ"
      },
      "source": [
        "def export_preprocessor(preprocessor_function, filepath):\n",
        "    import dill\n",
        "    with open(filepath, \"wb\") as f:\n",
        "        dill.dump(preprocessor_function, f)\n",
        "\n",
        "# import aimodelshare as ai # Once we can deploy this, we use it in lieu of the below.\n",
        "# ai.export_preprocessor(preprocessor, \"preprocessor.pkl\")\n",
        "\n",
        "export_preprocessor(preprocessor, \"preprocessor.pkl\")"
      ],
      "execution_count": 59,
      "outputs": []
    }
  ]
}