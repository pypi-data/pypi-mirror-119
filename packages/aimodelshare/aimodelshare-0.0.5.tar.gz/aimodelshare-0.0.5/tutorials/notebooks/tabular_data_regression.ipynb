{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "tabular_data_regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BviI4QIdWiq",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img width=\"50%\" src=\"https://aimodelsharecontent.s3.amazonaws.com/aimodshare_banner.jpg\" /></p>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYNp684Tk3tb",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><h1 align=\"center\">Boston Housing Prices Regression Tutorial</h1> <h3 align=\"center\">(Prepare to deploy model and preprocessor to REST API/Web Dashboard in four easy steps...)</h3></p>\n",
        "<p align=\"center\"><img width=\"80%\" src=\"https://aimodelsharecontent.s3.amazonaws.com/ModelandPreprocessorObjectPreparation.jpeg\" /></p>\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyskXyNqmBgy",
        "colab_type": "text"
      },
      "source": [
        "## **(1) Preprocessor Function & Setup**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjS1JhdVohXG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "0f2daaf1-a4cf-4e53-fba4-93199a732c89"
      },
      "source": [
        "! pip install scikit-learn --upgrade # Load newest version of sklearn."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 2.5MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.16.0)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.23.2 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfTZoC1t7gVO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "45b7a6bd-aa51-4a10-da5d-f43d84c1e501"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import datasets, linear_model\n",
        "\n",
        "# Obtaining the Boston Housing Prices dataset...\n",
        "boston = datasets.load_boston()\n",
        "X = pd.DataFrame(boston.data)\n",
        "\n",
        "X.columns = boston.feature_names\n",
        "y = boston.target # Or Price, i.e. median value of a house to be predicted.\n",
        "X.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.9</td>\n",
              "      <td>4.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.9</td>\n",
              "      <td>9.14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX  ...  RAD    TAX  PTRATIO      B  LSTAT\n",
              "0  0.00632  18.0   2.31   0.0  0.538  ...  1.0  296.0     15.3  396.9   4.98\n",
              "1  0.02731   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  396.9   9.14\n",
              "\n",
              "[2 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsW2O6OUW_N4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c1b5daf8-c6f0-4bb2-ad73-409427556cb8"
      },
      "source": [
        "# Set up training and test data...\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 1987)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X.columns.tolist())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(404, 13)\n",
            "(404,)\n",
            "['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6RBNdinfApF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# We create the preprocessing pipelines for both numeric and categorical data.\n",
        "numeric_features = ['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS',  'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "categorical_features = ['CHAS', 'RAD']\n",
        "\n",
        "# Replacing missing values with Modal value and then one hot encoding.\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Final preprocessor object set up with ColumnTransformer.\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "\n",
        "# Fit your preprocessor object.\n",
        "prediction_input_preprocessor=preprocessor.fit(X_train) \n",
        "\n",
        "import pickle\n",
        "pickle.dump(prediction_input_preprocessor, open(\"preprocessor.pkl\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQD978yOoZWu",
        "colab_type": "text"
      },
      "source": [
        "### **Write a Preprocessor Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs_6j1VEadXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is our preprocessor function to save using ai.export_preprocessor()...\n",
        "def preprocessor(data):\n",
        "  preprocessed_data=prediction_input_preprocessor.transform(data)\n",
        "  return preprocessed_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3KEaRaZW_N1",
        "colab_type": "text"
      },
      "source": [
        "## **(2) Build Your Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUGunoF3W_OE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d6962382-3282-4238-c4ee-286fe351e4e6"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_dim=22, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Dense(8))\n",
        "model.add(Dense(4))\n",
        "model.add(Dense(2))\n",
        "model.add(Dense(1))\n",
        "\n",
        "sgd = SGD(lr=0.001) # Learning rate for optimization set to 0.001.\n",
        "\n",
        "# Compile model...\n",
        "model.compile(loss='mse', optimizer=sgd, metrics=['mse'])\n",
        "\n",
        "# Fitting the ANN to the Training set...\n",
        "model.fit(preprocessor(X_train), y_train, \n",
        "               batch_size=32, \n",
        "               epochs=320, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/320\n",
            "13/13 - 0s - loss: 263.9630 - mse: 263.9630\n",
            "Epoch 2/320\n",
            "13/13 - 0s - loss: 134.9353 - mse: 134.9353\n",
            "Epoch 3/320\n",
            "13/13 - 0s - loss: 44.4583 - mse: 44.4583\n",
            "Epoch 4/320\n",
            "13/13 - 0s - loss: 54.6474 - mse: 54.6474\n",
            "Epoch 5/320\n",
            "13/13 - 0s - loss: 36.1740 - mse: 36.1740\n",
            "Epoch 6/320\n",
            "13/13 - 0s - loss: 36.6099 - mse: 36.6099\n",
            "Epoch 7/320\n",
            "13/13 - 0s - loss: 26.5400 - mse: 26.5400\n",
            "Epoch 8/320\n",
            "13/13 - 0s - loss: 41.3476 - mse: 41.3476\n",
            "Epoch 9/320\n",
            "13/13 - 0s - loss: 26.6015 - mse: 26.6015\n",
            "Epoch 10/320\n",
            "13/13 - 0s - loss: 35.7710 - mse: 35.7710\n",
            "Epoch 11/320\n",
            "13/13 - 0s - loss: 21.2827 - mse: 21.2827\n",
            "Epoch 12/320\n",
            "13/13 - 0s - loss: 15.9027 - mse: 15.9027\n",
            "Epoch 13/320\n",
            "13/13 - 0s - loss: 16.6077 - mse: 16.6077\n",
            "Epoch 14/320\n",
            "13/13 - 0s - loss: 20.4599 - mse: 20.4599\n",
            "Epoch 15/320\n",
            "13/13 - 0s - loss: 26.8827 - mse: 26.8827\n",
            "Epoch 16/320\n",
            "13/13 - 0s - loss: 16.5627 - mse: 16.5627\n",
            "Epoch 17/320\n",
            "13/13 - 0s - loss: 11.6181 - mse: 11.6181\n",
            "Epoch 18/320\n",
            "13/13 - 0s - loss: 7.8944 - mse: 7.8944\n",
            "Epoch 19/320\n",
            "13/13 - 0s - loss: 13.1227 - mse: 13.1227\n",
            "Epoch 20/320\n",
            "13/13 - 0s - loss: 15.2014 - mse: 15.2014\n",
            "Epoch 21/320\n",
            "13/13 - 0s - loss: 14.8414 - mse: 14.8414\n",
            "Epoch 22/320\n",
            "13/13 - 0s - loss: 11.3905 - mse: 11.3905\n",
            "Epoch 23/320\n",
            "13/13 - 0s - loss: 7.6721 - mse: 7.6721\n",
            "Epoch 24/320\n",
            "13/13 - 0s - loss: 12.3644 - mse: 12.3644\n",
            "Epoch 25/320\n",
            "13/13 - 0s - loss: 8.6421 - mse: 8.6421\n",
            "Epoch 26/320\n",
            "13/13 - 0s - loss: 5.9969 - mse: 5.9969\n",
            "Epoch 27/320\n",
            "13/13 - 0s - loss: 6.3070 - mse: 6.3070\n",
            "Epoch 28/320\n",
            "13/13 - 0s - loss: 9.7992 - mse: 9.7992\n",
            "Epoch 29/320\n",
            "13/13 - 0s - loss: 9.1325 - mse: 9.1325\n",
            "Epoch 30/320\n",
            "13/13 - 0s - loss: 6.4097 - mse: 6.4097\n",
            "Epoch 31/320\n",
            "13/13 - 0s - loss: 9.5807 - mse: 9.5807\n",
            "Epoch 32/320\n",
            "13/13 - 0s - loss: 8.1532 - mse: 8.1532\n",
            "Epoch 33/320\n",
            "13/13 - 0s - loss: 8.0101 - mse: 8.0101\n",
            "Epoch 34/320\n",
            "13/13 - 0s - loss: 6.7630 - mse: 6.7630\n",
            "Epoch 35/320\n",
            "13/13 - 0s - loss: 10.0578 - mse: 10.0578\n",
            "Epoch 36/320\n",
            "13/13 - 0s - loss: 12.2822 - mse: 12.2822\n",
            "Epoch 37/320\n",
            "13/13 - 0s - loss: 6.1679 - mse: 6.1679\n",
            "Epoch 38/320\n",
            "13/13 - 0s - loss: 6.7622 - mse: 6.7622\n",
            "Epoch 39/320\n",
            "13/13 - 0s - loss: 6.7328 - mse: 6.7328\n",
            "Epoch 40/320\n",
            "13/13 - 0s - loss: 7.3603 - mse: 7.3603\n",
            "Epoch 41/320\n",
            "13/13 - 0s - loss: 8.7107 - mse: 8.7107\n",
            "Epoch 42/320\n",
            "13/13 - 0s - loss: 6.4505 - mse: 6.4505\n",
            "Epoch 43/320\n",
            "13/13 - 0s - loss: 7.2626 - mse: 7.2626\n",
            "Epoch 44/320\n",
            "13/13 - 0s - loss: 6.9382 - mse: 6.9382\n",
            "Epoch 45/320\n",
            "13/13 - 0s - loss: 5.9975 - mse: 5.9975\n",
            "Epoch 46/320\n",
            "13/13 - 0s - loss: 5.3177 - mse: 5.3177\n",
            "Epoch 47/320\n",
            "13/13 - 0s - loss: 8.6470 - mse: 8.6470\n",
            "Epoch 48/320\n",
            "13/13 - 0s - loss: 9.4746 - mse: 9.4746\n",
            "Epoch 49/320\n",
            "13/13 - 0s - loss: 4.2328 - mse: 4.2328\n",
            "Epoch 50/320\n",
            "13/13 - 0s - loss: 4.2995 - mse: 4.2995\n",
            "Epoch 51/320\n",
            "13/13 - 0s - loss: 4.5484 - mse: 4.5484\n",
            "Epoch 52/320\n",
            "13/13 - 0s - loss: 7.3118 - mse: 7.3118\n",
            "Epoch 53/320\n",
            "13/13 - 0s - loss: 15.3780 - mse: 15.3780\n",
            "Epoch 54/320\n",
            "13/13 - 0s - loss: 6.1608 - mse: 6.1608\n",
            "Epoch 55/320\n",
            "13/13 - 0s - loss: 5.8516 - mse: 5.8516\n",
            "Epoch 56/320\n",
            "13/13 - 0s - loss: 8.3279 - mse: 8.3279\n",
            "Epoch 57/320\n",
            "13/13 - 0s - loss: 4.6505 - mse: 4.6505\n",
            "Epoch 58/320\n",
            "13/13 - 0s - loss: 5.1636 - mse: 5.1636\n",
            "Epoch 59/320\n",
            "13/13 - 0s - loss: 16.2957 - mse: 16.2957\n",
            "Epoch 60/320\n",
            "13/13 - 0s - loss: 4.7168 - mse: 4.7168\n",
            "Epoch 61/320\n",
            "13/13 - 0s - loss: 8.2036 - mse: 8.2036\n",
            "Epoch 62/320\n",
            "13/13 - 0s - loss: 6.7730 - mse: 6.7730\n",
            "Epoch 63/320\n",
            "13/13 - 0s - loss: 6.9181 - mse: 6.9181\n",
            "Epoch 64/320\n",
            "13/13 - 0s - loss: 5.1887 - mse: 5.1887\n",
            "Epoch 65/320\n",
            "13/13 - 0s - loss: 3.9225 - mse: 3.9225\n",
            "Epoch 66/320\n",
            "13/13 - 0s - loss: 3.6824 - mse: 3.6824\n",
            "Epoch 67/320\n",
            "13/13 - 0s - loss: 9.0124 - mse: 9.0124\n",
            "Epoch 68/320\n",
            "13/13 - 0s - loss: 4.3705 - mse: 4.3705\n",
            "Epoch 69/320\n",
            "13/13 - 0s - loss: 8.9707 - mse: 8.9707\n",
            "Epoch 70/320\n",
            "13/13 - 0s - loss: 3.5206 - mse: 3.5206\n",
            "Epoch 71/320\n",
            "13/13 - 0s - loss: 4.1391 - mse: 4.1391\n",
            "Epoch 72/320\n",
            "13/13 - 0s - loss: 3.5493 - mse: 3.5493\n",
            "Epoch 73/320\n",
            "13/13 - 0s - loss: 3.5981 - mse: 3.5981\n",
            "Epoch 74/320\n",
            "13/13 - 0s - loss: 3.7234 - mse: 3.7234\n",
            "Epoch 75/320\n",
            "13/13 - 0s - loss: 3.8988 - mse: 3.8988\n",
            "Epoch 76/320\n",
            "13/13 - 0s - loss: 4.4737 - mse: 4.4737\n",
            "Epoch 77/320\n",
            "13/13 - 0s - loss: 6.8916 - mse: 6.8916\n",
            "Epoch 78/320\n",
            "13/13 - 0s - loss: 12.7795 - mse: 12.7795\n",
            "Epoch 79/320\n",
            "13/13 - 0s - loss: 4.6234 - mse: 4.6234\n",
            "Epoch 80/320\n",
            "13/13 - 0s - loss: 3.0472 - mse: 3.0472\n",
            "Epoch 81/320\n",
            "13/13 - 0s - loss: 5.3594 - mse: 5.3594\n",
            "Epoch 82/320\n",
            "13/13 - 0s - loss: 8.8239 - mse: 8.8239\n",
            "Epoch 83/320\n",
            "13/13 - 0s - loss: 8.4830 - mse: 8.4830\n",
            "Epoch 84/320\n",
            "13/13 - 0s - loss: 4.2457 - mse: 4.2457\n",
            "Epoch 85/320\n",
            "13/13 - 0s - loss: 5.6797 - mse: 5.6797\n",
            "Epoch 86/320\n",
            "13/13 - 0s - loss: 4.3064 - mse: 4.3064\n",
            "Epoch 87/320\n",
            "13/13 - 0s - loss: 3.9413 - mse: 3.9413\n",
            "Epoch 88/320\n",
            "13/13 - 0s - loss: 2.7172 - mse: 2.7172\n",
            "Epoch 89/320\n",
            "13/13 - 0s - loss: 6.1215 - mse: 6.1215\n",
            "Epoch 90/320\n",
            "13/13 - 0s - loss: 3.7336 - mse: 3.7336\n",
            "Epoch 91/320\n",
            "13/13 - 0s - loss: 2.8117 - mse: 2.8117\n",
            "Epoch 92/320\n",
            "13/13 - 0s - loss: 2.6610 - mse: 2.6610\n",
            "Epoch 93/320\n",
            "13/13 - 0s - loss: 3.9038 - mse: 3.9038\n",
            "Epoch 94/320\n",
            "13/13 - 0s - loss: 5.0570 - mse: 5.0570\n",
            "Epoch 95/320\n",
            "13/13 - 0s - loss: 3.4827 - mse: 3.4827\n",
            "Epoch 96/320\n",
            "13/13 - 0s - loss: 3.5573 - mse: 3.5573\n",
            "Epoch 97/320\n",
            "13/13 - 0s - loss: 4.2470 - mse: 4.2470\n",
            "Epoch 98/320\n",
            "13/13 - 0s - loss: 4.7693 - mse: 4.7693\n",
            "Epoch 99/320\n",
            "13/13 - 0s - loss: 5.1437 - mse: 5.1437\n",
            "Epoch 100/320\n",
            "13/13 - 0s - loss: 5.0520 - mse: 5.0520\n",
            "Epoch 101/320\n",
            "13/13 - 0s - loss: 4.3069 - mse: 4.3069\n",
            "Epoch 102/320\n",
            "13/13 - 0s - loss: 3.3440 - mse: 3.3440\n",
            "Epoch 103/320\n",
            "13/13 - 0s - loss: 6.3590 - mse: 6.3590\n",
            "Epoch 104/320\n",
            "13/13 - 0s - loss: 3.1562 - mse: 3.1562\n",
            "Epoch 105/320\n",
            "13/13 - 0s - loss: 2.6792 - mse: 2.6792\n",
            "Epoch 106/320\n",
            "13/13 - 0s - loss: 3.5598 - mse: 3.5598\n",
            "Epoch 107/320\n",
            "13/13 - 0s - loss: 3.8032 - mse: 3.8032\n",
            "Epoch 108/320\n",
            "13/13 - 0s - loss: 5.2990 - mse: 5.2990\n",
            "Epoch 109/320\n",
            "13/13 - 0s - loss: 3.3790 - mse: 3.3790\n",
            "Epoch 110/320\n",
            "13/13 - 0s - loss: 2.5229 - mse: 2.5229\n",
            "Epoch 111/320\n",
            "13/13 - 0s - loss: 2.5209 - mse: 2.5209\n",
            "Epoch 112/320\n",
            "13/13 - 0s - loss: 2.6972 - mse: 2.6972\n",
            "Epoch 113/320\n",
            "13/13 - 0s - loss: 3.6095 - mse: 3.6095\n",
            "Epoch 114/320\n",
            "13/13 - 0s - loss: 2.3499 - mse: 2.3499\n",
            "Epoch 115/320\n",
            "13/13 - 0s - loss: 3.3672 - mse: 3.3672\n",
            "Epoch 116/320\n",
            "13/13 - 0s - loss: 2.3202 - mse: 2.3202\n",
            "Epoch 117/320\n",
            "13/13 - 0s - loss: 4.5303 - mse: 4.5303\n",
            "Epoch 118/320\n",
            "13/13 - 0s - loss: 2.2162 - mse: 2.2162\n",
            "Epoch 119/320\n",
            "13/13 - 0s - loss: 3.3550 - mse: 3.3550\n",
            "Epoch 120/320\n",
            "13/13 - 0s - loss: 5.7197 - mse: 5.7197\n",
            "Epoch 121/320\n",
            "13/13 - 0s - loss: 3.3399 - mse: 3.3399\n",
            "Epoch 122/320\n",
            "13/13 - 0s - loss: 2.3134 - mse: 2.3134\n",
            "Epoch 123/320\n",
            "13/13 - 0s - loss: 7.2312 - mse: 7.2312\n",
            "Epoch 124/320\n",
            "13/13 - 0s - loss: 2.7118 - mse: 2.7118\n",
            "Epoch 125/320\n",
            "13/13 - 0s - loss: 6.0928 - mse: 6.0928\n",
            "Epoch 126/320\n",
            "13/13 - 0s - loss: 5.1525 - mse: 5.1525\n",
            "Epoch 127/320\n",
            "13/13 - 0s - loss: 2.6370 - mse: 2.6370\n",
            "Epoch 128/320\n",
            "13/13 - 0s - loss: 2.6736 - mse: 2.6736\n",
            "Epoch 129/320\n",
            "13/13 - 0s - loss: 2.2730 - mse: 2.2730\n",
            "Epoch 130/320\n",
            "13/13 - 0s - loss: 4.7895 - mse: 4.7895\n",
            "Epoch 131/320\n",
            "13/13 - 0s - loss: 6.1335 - mse: 6.1335\n",
            "Epoch 132/320\n",
            "13/13 - 0s - loss: 2.9059 - mse: 2.9059\n",
            "Epoch 133/320\n",
            "13/13 - 0s - loss: 2.8994 - mse: 2.8994\n",
            "Epoch 134/320\n",
            "13/13 - 0s - loss: 4.1195 - mse: 4.1195\n",
            "Epoch 135/320\n",
            "13/13 - 0s - loss: 3.0824 - mse: 3.0824\n",
            "Epoch 136/320\n",
            "13/13 - 0s - loss: 6.5381 - mse: 6.5381\n",
            "Epoch 137/320\n",
            "13/13 - 0s - loss: 3.6954 - mse: 3.6954\n",
            "Epoch 138/320\n",
            "13/13 - 0s - loss: 6.5121 - mse: 6.5121\n",
            "Epoch 139/320\n",
            "13/13 - 0s - loss: 2.6186 - mse: 2.6186\n",
            "Epoch 140/320\n",
            "13/13 - 0s - loss: 2.8997 - mse: 2.8997\n",
            "Epoch 141/320\n",
            "13/13 - 0s - loss: 2.9254 - mse: 2.9254\n",
            "Epoch 142/320\n",
            "13/13 - 0s - loss: 5.9649 - mse: 5.9649\n",
            "Epoch 143/320\n",
            "13/13 - 0s - loss: 8.0712 - mse: 8.0712\n",
            "Epoch 144/320\n",
            "13/13 - 0s - loss: 2.6421 - mse: 2.6421\n",
            "Epoch 145/320\n",
            "13/13 - 0s - loss: 2.8359 - mse: 2.8359\n",
            "Epoch 146/320\n",
            "13/13 - 0s - loss: 2.5915 - mse: 2.5915\n",
            "Epoch 147/320\n",
            "13/13 - 0s - loss: 2.1773 - mse: 2.1773\n",
            "Epoch 148/320\n",
            "13/13 - 0s - loss: 4.9924 - mse: 4.9924\n",
            "Epoch 149/320\n",
            "13/13 - 0s - loss: 2.1575 - mse: 2.1575\n",
            "Epoch 150/320\n",
            "13/13 - 0s - loss: 3.7827 - mse: 3.7827\n",
            "Epoch 151/320\n",
            "13/13 - 0s - loss: 2.0403 - mse: 2.0403\n",
            "Epoch 152/320\n",
            "13/13 - 0s - loss: 3.8613 - mse: 3.8613\n",
            "Epoch 153/320\n",
            "13/13 - 0s - loss: 2.8978 - mse: 2.8978\n",
            "Epoch 154/320\n",
            "13/13 - 0s - loss: 2.5510 - mse: 2.5510\n",
            "Epoch 155/320\n",
            "13/13 - 0s - loss: 1.7610 - mse: 1.7610\n",
            "Epoch 156/320\n",
            "13/13 - 0s - loss: 4.9900 - mse: 4.9900\n",
            "Epoch 157/320\n",
            "13/13 - 0s - loss: 2.4182 - mse: 2.4182\n",
            "Epoch 158/320\n",
            "13/13 - 0s - loss: 2.5354 - mse: 2.5354\n",
            "Epoch 159/320\n",
            "13/13 - 0s - loss: 2.0533 - mse: 2.0533\n",
            "Epoch 160/320\n",
            "13/13 - 0s - loss: 2.7819 - mse: 2.7819\n",
            "Epoch 161/320\n",
            "13/13 - 0s - loss: 2.3165 - mse: 2.3165\n",
            "Epoch 162/320\n",
            "13/13 - 0s - loss: 1.8420 - mse: 1.8420\n",
            "Epoch 163/320\n",
            "13/13 - 0s - loss: 4.3161 - mse: 4.3161\n",
            "Epoch 164/320\n",
            "13/13 - 0s - loss: 3.3957 - mse: 3.3957\n",
            "Epoch 165/320\n",
            "13/13 - 0s - loss: 1.9976 - mse: 1.9976\n",
            "Epoch 166/320\n",
            "13/13 - 0s - loss: 3.8592 - mse: 3.8592\n",
            "Epoch 167/320\n",
            "13/13 - 0s - loss: 3.3663 - mse: 3.3663\n",
            "Epoch 168/320\n",
            "13/13 - 0s - loss: 1.9812 - mse: 1.9812\n",
            "Epoch 169/320\n",
            "13/13 - 0s - loss: 3.1586 - mse: 3.1586\n",
            "Epoch 170/320\n",
            "13/13 - 0s - loss: 2.5786 - mse: 2.5786\n",
            "Epoch 171/320\n",
            "13/13 - 0s - loss: 6.1679 - mse: 6.1679\n",
            "Epoch 172/320\n",
            "13/13 - 0s - loss: 6.9925 - mse: 6.9925\n",
            "Epoch 173/320\n",
            "13/13 - 0s - loss: 4.3023 - mse: 4.3023\n",
            "Epoch 174/320\n",
            "13/13 - 0s - loss: 4.6656 - mse: 4.6656\n",
            "Epoch 175/320\n",
            "13/13 - 0s - loss: 2.6066 - mse: 2.6066\n",
            "Epoch 176/320\n",
            "13/13 - 0s - loss: 2.5654 - mse: 2.5654\n",
            "Epoch 177/320\n",
            "13/13 - 0s - loss: 2.3330 - mse: 2.3330\n",
            "Epoch 178/320\n",
            "13/13 - 0s - loss: 2.4441 - mse: 2.4441\n",
            "Epoch 179/320\n",
            "13/13 - 0s - loss: 1.7420 - mse: 1.7420\n",
            "Epoch 180/320\n",
            "13/13 - 0s - loss: 1.6182 - mse: 1.6182\n",
            "Epoch 181/320\n",
            "13/13 - 0s - loss: 1.5834 - mse: 1.5834\n",
            "Epoch 182/320\n",
            "13/13 - 0s - loss: 2.4741 - mse: 2.4741\n",
            "Epoch 183/320\n",
            "13/13 - 0s - loss: 1.6540 - mse: 1.6540\n",
            "Epoch 184/320\n",
            "13/13 - 0s - loss: 1.6832 - mse: 1.6832\n",
            "Epoch 185/320\n",
            "13/13 - 0s - loss: 1.5517 - mse: 1.5517\n",
            "Epoch 186/320\n",
            "13/13 - 0s - loss: 3.7826 - mse: 3.7826\n",
            "Epoch 187/320\n",
            "13/13 - 0s - loss: 2.8729 - mse: 2.8729\n",
            "Epoch 188/320\n",
            "13/13 - 0s - loss: 2.9651 - mse: 2.9651\n",
            "Epoch 189/320\n",
            "13/13 - 0s - loss: 2.1788 - mse: 2.1788\n",
            "Epoch 190/320\n",
            "13/13 - 0s - loss: 1.8168 - mse: 1.8168\n",
            "Epoch 191/320\n",
            "13/13 - 0s - loss: 2.9115 - mse: 2.9115\n",
            "Epoch 192/320\n",
            "13/13 - 0s - loss: 3.8434 - mse: 3.8434\n",
            "Epoch 193/320\n",
            "13/13 - 0s - loss: 7.1719 - mse: 7.1719\n",
            "Epoch 194/320\n",
            "13/13 - 0s - loss: 1.6362 - mse: 1.6362\n",
            "Epoch 195/320\n",
            "13/13 - 0s - loss: 1.5521 - mse: 1.5521\n",
            "Epoch 196/320\n",
            "13/13 - 0s - loss: 5.5732 - mse: 5.5732\n",
            "Epoch 197/320\n",
            "13/13 - 0s - loss: 1.9068 - mse: 1.9068\n",
            "Epoch 198/320\n",
            "13/13 - 0s - loss: 3.4523 - mse: 3.4523\n",
            "Epoch 199/320\n",
            "13/13 - 0s - loss: 7.9808 - mse: 7.9808\n",
            "Epoch 200/320\n",
            "13/13 - 0s - loss: 1.5510 - mse: 1.5510\n",
            "Epoch 201/320\n",
            "13/13 - 0s - loss: 2.4201 - mse: 2.4201\n",
            "Epoch 202/320\n",
            "13/13 - 0s - loss: 1.9134 - mse: 1.9134\n",
            "Epoch 203/320\n",
            "13/13 - 0s - loss: 1.3869 - mse: 1.3869\n",
            "Epoch 204/320\n",
            "13/13 - 0s - loss: 1.8318 - mse: 1.8318\n",
            "Epoch 205/320\n",
            "13/13 - 0s - loss: 1.4933 - mse: 1.4933\n",
            "Epoch 206/320\n",
            "13/13 - 0s - loss: 2.2989 - mse: 2.2989\n",
            "Epoch 207/320\n",
            "13/13 - 0s - loss: 4.3462 - mse: 4.3462\n",
            "Epoch 208/320\n",
            "13/13 - 0s - loss: 2.6600 - mse: 2.6600\n",
            "Epoch 209/320\n",
            "13/13 - 0s - loss: 2.0389 - mse: 2.0389\n",
            "Epoch 210/320\n",
            "13/13 - 0s - loss: 2.6933 - mse: 2.6933\n",
            "Epoch 211/320\n",
            "13/13 - 0s - loss: 2.0481 - mse: 2.0481\n",
            "Epoch 212/320\n",
            "13/13 - 0s - loss: 1.7376 - mse: 1.7376\n",
            "Epoch 213/320\n",
            "13/13 - 0s - loss: 1.3374 - mse: 1.3374\n",
            "Epoch 214/320\n",
            "13/13 - 0s - loss: 2.3783 - mse: 2.3783\n",
            "Epoch 215/320\n",
            "13/13 - 0s - loss: 1.4299 - mse: 1.4299\n",
            "Epoch 216/320\n",
            "13/13 - 0s - loss: 1.8024 - mse: 1.8024\n",
            "Epoch 217/320\n",
            "13/13 - 0s - loss: 1.4617 - mse: 1.4617\n",
            "Epoch 218/320\n",
            "13/13 - 0s - loss: 2.9152 - mse: 2.9152\n",
            "Epoch 219/320\n",
            "13/13 - 0s - loss: 2.4646 - mse: 2.4646\n",
            "Epoch 220/320\n",
            "13/13 - 0s - loss: 1.4629 - mse: 1.4629\n",
            "Epoch 221/320\n",
            "13/13 - 0s - loss: 2.9474 - mse: 2.9474\n",
            "Epoch 222/320\n",
            "13/13 - 0s - loss: 1.7765 - mse: 1.7765\n",
            "Epoch 223/320\n",
            "13/13 - 0s - loss: 2.0971 - mse: 2.0971\n",
            "Epoch 224/320\n",
            "13/13 - 0s - loss: 1.7581 - mse: 1.7581\n",
            "Epoch 225/320\n",
            "13/13 - 0s - loss: 1.4709 - mse: 1.4709\n",
            "Epoch 226/320\n",
            "13/13 - 0s - loss: 1.3555 - mse: 1.3555\n",
            "Epoch 227/320\n",
            "13/13 - 0s - loss: 2.2025 - mse: 2.2025\n",
            "Epoch 228/320\n",
            "13/13 - 0s - loss: 1.7852 - mse: 1.7852\n",
            "Epoch 229/320\n",
            "13/13 - 0s - loss: 1.7081 - mse: 1.7081\n",
            "Epoch 230/320\n",
            "13/13 - 0s - loss: 2.4181 - mse: 2.4181\n",
            "Epoch 231/320\n",
            "13/13 - 0s - loss: 3.5012 - mse: 3.5012\n",
            "Epoch 232/320\n",
            "13/13 - 0s - loss: 1.5746 - mse: 1.5746\n",
            "Epoch 233/320\n",
            "13/13 - 0s - loss: 1.6345 - mse: 1.6345\n",
            "Epoch 234/320\n",
            "13/13 - 0s - loss: 2.9171 - mse: 2.9171\n",
            "Epoch 235/320\n",
            "13/13 - 0s - loss: 1.6894 - mse: 1.6894\n",
            "Epoch 236/320\n",
            "13/13 - 0s - loss: 2.7756 - mse: 2.7756\n",
            "Epoch 237/320\n",
            "13/13 - 0s - loss: 1.3161 - mse: 1.3161\n",
            "Epoch 238/320\n",
            "13/13 - 0s - loss: 1.2438 - mse: 1.2438\n",
            "Epoch 239/320\n",
            "13/13 - 0s - loss: 2.3595 - mse: 2.3595\n",
            "Epoch 240/320\n",
            "13/13 - 0s - loss: 2.1357 - mse: 2.1357\n",
            "Epoch 241/320\n",
            "13/13 - 0s - loss: 1.2773 - mse: 1.2773\n",
            "Epoch 242/320\n",
            "13/13 - 0s - loss: 1.3115 - mse: 1.3115\n",
            "Epoch 243/320\n",
            "13/13 - 0s - loss: 1.1412 - mse: 1.1412\n",
            "Epoch 244/320\n",
            "13/13 - 0s - loss: 1.3650 - mse: 1.3650\n",
            "Epoch 245/320\n",
            "13/13 - 0s - loss: 1.4286 - mse: 1.4286\n",
            "Epoch 246/320\n",
            "13/13 - 0s - loss: 2.2981 - mse: 2.2981\n",
            "Epoch 247/320\n",
            "13/13 - 0s - loss: 2.4889 - mse: 2.4889\n",
            "Epoch 248/320\n",
            "13/13 - 0s - loss: 3.2109 - mse: 3.2109\n",
            "Epoch 249/320\n",
            "13/13 - 0s - loss: 1.2794 - mse: 1.2794\n",
            "Epoch 250/320\n",
            "13/13 - 0s - loss: 1.7493 - mse: 1.7493\n",
            "Epoch 251/320\n",
            "13/13 - 0s - loss: 3.6298 - mse: 3.6298\n",
            "Epoch 252/320\n",
            "13/13 - 0s - loss: 4.8242 - mse: 4.8242\n",
            "Epoch 253/320\n",
            "13/13 - 0s - loss: 1.4565 - mse: 1.4565\n",
            "Epoch 254/320\n",
            "13/13 - 0s - loss: 1.3833 - mse: 1.3833\n",
            "Epoch 255/320\n",
            "13/13 - 0s - loss: 1.4290 - mse: 1.4290\n",
            "Epoch 256/320\n",
            "13/13 - 0s - loss: 1.9734 - mse: 1.9734\n",
            "Epoch 257/320\n",
            "13/13 - 0s - loss: 1.7451 - mse: 1.7451\n",
            "Epoch 258/320\n",
            "13/13 - 0s - loss: 1.3669 - mse: 1.3669\n",
            "Epoch 259/320\n",
            "13/13 - 0s - loss: 3.4799 - mse: 3.4799\n",
            "Epoch 260/320\n",
            "13/13 - 0s - loss: 1.3560 - mse: 1.3560\n",
            "Epoch 261/320\n",
            "13/13 - 0s - loss: 1.6737 - mse: 1.6737\n",
            "Epoch 262/320\n",
            "13/13 - 0s - loss: 2.2795 - mse: 2.2795\n",
            "Epoch 263/320\n",
            "13/13 - 0s - loss: 1.4642 - mse: 1.4642\n",
            "Epoch 264/320\n",
            "13/13 - 0s - loss: 1.0575 - mse: 1.0575\n",
            "Epoch 265/320\n",
            "13/13 - 0s - loss: 1.0532 - mse: 1.0532\n",
            "Epoch 266/320\n",
            "13/13 - 0s - loss: 1.0619 - mse: 1.0619\n",
            "Epoch 267/320\n",
            "13/13 - 0s - loss: 7.0992 - mse: 7.0992\n",
            "Epoch 268/320\n",
            "13/13 - 0s - loss: 1.2870 - mse: 1.2870\n",
            "Epoch 269/320\n",
            "13/13 - 0s - loss: 1.2048 - mse: 1.2048\n",
            "Epoch 270/320\n",
            "13/13 - 0s - loss: 1.3792 - mse: 1.3792\n",
            "Epoch 271/320\n",
            "13/13 - 0s - loss: 4.0120 - mse: 4.0120\n",
            "Epoch 272/320\n",
            "13/13 - 0s - loss: 1.6661 - mse: 1.6661\n",
            "Epoch 273/320\n",
            "13/13 - 0s - loss: 2.6264 - mse: 2.6264\n",
            "Epoch 274/320\n",
            "13/13 - 0s - loss: 5.3833 - mse: 5.3833\n",
            "Epoch 275/320\n",
            "13/13 - 0s - loss: 1.3170 - mse: 1.3170\n",
            "Epoch 276/320\n",
            "13/13 - 0s - loss: 2.7099 - mse: 2.7099\n",
            "Epoch 277/320\n",
            "13/13 - 0s - loss: 1.9375 - mse: 1.9375\n",
            "Epoch 278/320\n",
            "13/13 - 0s - loss: 1.4492 - mse: 1.4492\n",
            "Epoch 279/320\n",
            "13/13 - 0s - loss: 1.2994 - mse: 1.2994\n",
            "Epoch 280/320\n",
            "13/13 - 0s - loss: 1.1097 - mse: 1.1097\n",
            "Epoch 281/320\n",
            "13/13 - 0s - loss: 3.6762 - mse: 3.6762\n",
            "Epoch 282/320\n",
            "13/13 - 0s - loss: 1.1780 - mse: 1.1780\n",
            "Epoch 283/320\n",
            "13/13 - 0s - loss: 1.2637 - mse: 1.2637\n",
            "Epoch 284/320\n",
            "13/13 - 0s - loss: 1.1245 - mse: 1.1245\n",
            "Epoch 285/320\n",
            "13/13 - 0s - loss: 2.1493 - mse: 2.1493\n",
            "Epoch 286/320\n",
            "13/13 - 0s - loss: 1.7387 - mse: 1.7387\n",
            "Epoch 287/320\n",
            "13/13 - 0s - loss: 2.9039 - mse: 2.9039\n",
            "Epoch 288/320\n",
            "13/13 - 0s - loss: 1.7737 - mse: 1.7737\n",
            "Epoch 289/320\n",
            "13/13 - 0s - loss: 1.0515 - mse: 1.0515\n",
            "Epoch 290/320\n",
            "13/13 - 0s - loss: 0.9650 - mse: 0.9650\n",
            "Epoch 291/320\n",
            "13/13 - 0s - loss: 3.7312 - mse: 3.7312\n",
            "Epoch 292/320\n",
            "13/13 - 0s - loss: 2.8983 - mse: 2.8983\n",
            "Epoch 293/320\n",
            "13/13 - 0s - loss: 1.2660 - mse: 1.2660\n",
            "Epoch 294/320\n",
            "13/13 - 0s - loss: 0.9533 - mse: 0.9533\n",
            "Epoch 295/320\n",
            "13/13 - 0s - loss: 0.9602 - mse: 0.9602\n",
            "Epoch 296/320\n",
            "13/13 - 0s - loss: 1.6625 - mse: 1.6625\n",
            "Epoch 297/320\n",
            "13/13 - 0s - loss: 0.9767 - mse: 0.9767\n",
            "Epoch 298/320\n",
            "13/13 - 0s - loss: 0.9062 - mse: 0.9062\n",
            "Epoch 299/320\n",
            "13/13 - 0s - loss: 0.9343 - mse: 0.9343\n",
            "Epoch 300/320\n",
            "13/13 - 0s - loss: 4.4568 - mse: 4.4568\n",
            "Epoch 301/320\n",
            "13/13 - 0s - loss: 9.7165 - mse: 9.7165\n",
            "Epoch 302/320\n",
            "13/13 - 0s - loss: 1.1664 - mse: 1.1664\n",
            "Epoch 303/320\n",
            "13/13 - 0s - loss: 1.6324 - mse: 1.6324\n",
            "Epoch 304/320\n",
            "13/13 - 0s - loss: 1.6217 - mse: 1.6217\n",
            "Epoch 305/320\n",
            "13/13 - 0s - loss: 1.2088 - mse: 1.2088\n",
            "Epoch 306/320\n",
            "13/13 - 0s - loss: 1.3767 - mse: 1.3767\n",
            "Epoch 307/320\n",
            "13/13 - 0s - loss: 1.0401 - mse: 1.0401\n",
            "Epoch 308/320\n",
            "13/13 - 0s - loss: 1.5258 - mse: 1.5258\n",
            "Epoch 309/320\n",
            "13/13 - 0s - loss: 1.2161 - mse: 1.2161\n",
            "Epoch 310/320\n",
            "13/13 - 0s - loss: 1.1177 - mse: 1.1177\n",
            "Epoch 311/320\n",
            "13/13 - 0s - loss: 0.9444 - mse: 0.9444\n",
            "Epoch 312/320\n",
            "13/13 - 0s - loss: 0.9077 - mse: 0.9077\n",
            "Epoch 313/320\n",
            "13/13 - 0s - loss: 1.0015 - mse: 1.0015\n",
            "Epoch 314/320\n",
            "13/13 - 0s - loss: 1.3202 - mse: 1.3202\n",
            "Epoch 315/320\n",
            "13/13 - 0s - loss: 1.3753 - mse: 1.3753\n",
            "Epoch 316/320\n",
            "13/13 - 0s - loss: 1.0327 - mse: 1.0327\n",
            "Epoch 317/320\n",
            "13/13 - 0s - loss: 0.8786 - mse: 0.8786\n",
            "Epoch 318/320\n",
            "13/13 - 0s - loss: 0.8926 - mse: 0.8926\n",
            "Epoch 319/320\n",
            "13/13 - 0s - loss: 2.0607 - mse: 2.0607\n",
            "Epoch 320/320\n",
            "13/13 - 0s - loss: 4.0792 - mse: 4.0792\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f405660b1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDlYvRxmqlw5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "46b48cdc-dea9-4db7-9fb9-69b10307cccf"
      },
      "source": [
        "model.predict(preprocessor(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[21.993082],\n",
              "       [21.761253],\n",
              "       [10.310246],\n",
              "       [25.950556],\n",
              "       [19.207293],\n",
              "       [23.505026],\n",
              "       [19.505232],\n",
              "       [13.390075],\n",
              "       [46.56502 ],\n",
              "       [18.026236],\n",
              "       [21.36947 ],\n",
              "       [20.193367],\n",
              "       [15.167277],\n",
              "       [ 9.763607],\n",
              "       [17.742172],\n",
              "       [22.325336],\n",
              "       [11.052958],\n",
              "       [22.89468 ],\n",
              "       [19.418995],\n",
              "       [24.387182],\n",
              "       [20.781782],\n",
              "       [29.231243],\n",
              "       [ 9.282041],\n",
              "       [18.531254],\n",
              "       [21.965158],\n",
              "       [27.697096],\n",
              "       [17.572123],\n",
              "       [20.194084],\n",
              "       [11.715147],\n",
              "       [18.63393 ],\n",
              "       [26.625349],\n",
              "       [13.789065],\n",
              "       [15.539425],\n",
              "       [28.377356],\n",
              "       [16.78662 ],\n",
              "       [15.345576],\n",
              "       [15.843083],\n",
              "       [25.002707],\n",
              "       [19.726068],\n",
              "       [18.174925],\n",
              "       [25.865255],\n",
              "       [21.9546  ],\n",
              "       [18.482027],\n",
              "       [14.168027],\n",
              "       [21.37304 ],\n",
              "       [13.39111 ],\n",
              "       [20.691542],\n",
              "       [21.515924],\n",
              "       [28.338057],\n",
              "       [11.8245  ],\n",
              "       [27.480896],\n",
              "       [20.452503],\n",
              "       [20.555393],\n",
              "       [26.116749],\n",
              "       [19.854038],\n",
              "       [16.097603],\n",
              "       [10.464519],\n",
              "       [36.818268],\n",
              "       [17.415873],\n",
              "       [21.846455],\n",
              "       [20.623896],\n",
              "       [34.38792 ],\n",
              "       [33.647625],\n",
              "       [43.125473],\n",
              "       [23.473623],\n",
              "       [29.360518],\n",
              "       [24.33833 ],\n",
              "       [19.293428],\n",
              "       [13.538396],\n",
              "       [15.289931],\n",
              "       [19.926266],\n",
              "       [24.131292],\n",
              "       [15.411683],\n",
              "       [24.757776],\n",
              "       [25.334614],\n",
              "       [22.637165],\n",
              "       [20.429785],\n",
              "       [15.625414],\n",
              "       [19.072523],\n",
              "       [ 8.051111],\n",
              "       [11.275405],\n",
              "       [11.498287],\n",
              "       [24.621624],\n",
              "       [21.749691],\n",
              "       [18.38091 ],\n",
              "       [34.02953 ],\n",
              "       [21.340954],\n",
              "       [22.280716],\n",
              "       [46.928806],\n",
              "       [10.97075 ],\n",
              "       [17.32509 ],\n",
              "       [28.878246],\n",
              "       [32.089622],\n",
              "       [18.316675],\n",
              "       [14.1048  ],\n",
              "       [16.146353],\n",
              "       [17.072838],\n",
              "       [18.055014],\n",
              "       [ 9.023691],\n",
              "       [21.563074],\n",
              "       [25.855688],\n",
              "       [14.825657]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFmO3bEwf50R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4188e15b-0f96-409b-e04f-1f7905109d05"
      },
      "source": [
        "y_pred = model.predict(preprocessor(X_test))\n",
        "\n",
        "score = model.evaluate(preprocessor(X_test), y_test)\n",
        "print('Accuracy: {:4f}'.format(score[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 2ms/step - loss: 10.0303 - mse: 10.0303\n",
            "Accuracy: 10.030313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA3j4pFj6_nl",
        "colab_type": "text"
      },
      "source": [
        "## **(3) Save Preprocessor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX7H1LvL6_7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! pip3 install aimodelshare"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H5kjnbg7AO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def export_preprocessor(preprocessor_function, filepath):\n",
        "    import dill\n",
        "    with open(filepath, \"wb\") as f:\n",
        "        dill.dump(preprocessor_function, f)\n",
        "\n",
        "# import aimodelshare as ai # Once we can deploy this, we use it in lieu of the below.\n",
        "# ai.export_preprocessor(preprocessor, \"preprocessor.pkl\")\n",
        "\n",
        "export_preprocessor(preprocessor, \"preprocessor.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ4AKVvh6i38",
        "colab_type": "text"
      },
      "source": [
        "## **(4) Save Keras Model to Onnx File Format**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j741h11LvmYa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "3a89cc06-9c6d-4d7d-af94-18b670eb504c"
      },
      "source": [
        "! pip3 install keras2onnx\n",
        "! pip3 install onnxruntime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras2onnx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/2f/c7aef8f8215c62d55ea05f5b36737c1726e4fea6c73970909523ae497fd9/keras2onnx-1.7.0-py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (1.18.5)\n",
            "Collecting onnxconverter-common>=1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/7a/7e30c643cd7d2ad87689188ef34ce93e657bd14da3605f87bcdbc19cd5b1/onnxconverter_common-1.7.0-py2.py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.5MB/s \n",
            "\u001b[?25hCollecting onnx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/ee/bc7bc88fc8449266add978627e90c363069211584b937fd867b0ccc59f09/onnx-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (3.12.4)\n",
            "Collecting fire\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (2020.6.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnx->keras2onnx) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx->keras2onnx) (3.7.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->keras2onnx) (49.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire->keras2onnx) (1.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=498d65e51db6254a7b8923d116826e5990587fcde8fd428e7f590dc27608de91\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n",
            "Successfully built fire\n",
            "Installing collected packages: onnx, onnxconverter-common, fire, keras2onnx\n",
            "Successfully installed fire-0.3.1 keras2onnx-1.7.0 onnx-1.7.0 onnxconverter-common-1.7.0\n",
            "Collecting onnxruntime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/a6/30c6c17524d6930df677412eda0b3c2acd9062697fc0e86842f2dd058835/onnxruntime-1.4.0-cp36-cp36m-manylinux2010_x86_64.whl (4.4MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnxruntime) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from onnxruntime) (1.18.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime) (49.2.0)\n",
            "Installing collected packages: onnxruntime\n",
            "Successfully installed onnxruntime-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1382KctFaL8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "26079f69-bb78-4856-88fd-4418db1630ac"
      },
      "source": [
        "# Save model to onnx file...\n",
        "\n",
        "import os\n",
        "os.environ['TF_KERAS'] = '1' # Add this environmental variable whenever you use tensorflow's tf.keras to build your keras model.\n",
        "\n",
        "import onnx\n",
        "import keras2onnx\n",
        "\n",
        "# Convert model to onnx object\n",
        "import onnx\n",
        "from keras2onnx import convert_keras\n",
        "onnx_model = convert_keras(model, 'my_model.onnx')\n",
        "\n",
        "# Save model to local .onnx file\n",
        "with open(\"my_model.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf executing eager_mode: True\n",
            "tf.keras model eager_mode: False\n",
            "The ONNX operator number change on the optimization: 25 -> 16\n",
            "The maximum opset needed by this model is only 9.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
